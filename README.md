# ğŸŒ Webscraping-By-R

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)  
[![R Version](https://img.shields.io/badge/R-%3E%3D%204.0.0-blue.svg)](https://cran.r-project.org/)  
[![Made with R](https://img.shields.io/badge/Made%20with-R-blue?logo=r&logoColor=white)](https://www.r-project.org/)

---

## ğŸ“– Introduction

This project demonstrates **web scraping** in R to extract **global COVID-19 case data** from Wikipedia.  
The scraped data is **cleaned, saved to CSV, and visualized** using bar charts for quick interpretation.  

**Why this project?**  
In the era of big data, being able to automatically extract, process, and visualize information from the web is an essential skill for data scientists and analysts.

---

## ğŸ›  Technologies Used

- **[R](https://www.r-project.org/)** â€“ Main programming language  
- **[rvest](https://cran.r-project.org/web/packages/rvest/index.html)** â€“ For web scraping  
- **[dplyr](https://dplyr.tidyverse.org/)** â€“ For data manipulation  
- **[ggplot2](https://ggplot2.tidyverse.org/)** â€“ For data visualization  

---

## ğŸš€ Features

âœ” Extracts up-to-date COVID-19 case numbers from Wikipedia  
âœ” Cleans and formats the dataset  
âœ” Saves results locally as CSV  
âœ” Generates a bar chart for visual insights  

---

## ğŸ“· Preview Output

**Example Visualization**:  
![COVID-19 Chart Preview](assets/preview_chart.png)  
*(Replace this image with your actual plot output)*  

---

## ğŸ“‹ Example Output Data

| Country     | Cases       |
|-------------|------------|
| USA         | 103,456,789|
| India       | 45,678,123 |
| Brazil      | 34,567,890 |
| France      | 29,345,678 |

*(Values above are just sample data â€” actual values come from Wikipedia.)*

---


## â–¶ï¸ Run the Project in Google Colab

You can run this project directly in your browser without installing anything:  
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](YOUR_COLAB_NOTEBOOK_LINK)



## âš™ï¸ How to Run
1. Load the libraries

```r
library(rvest)
library(dplyr)
library(ggplot2)
```
2. Read the Wikipedia page
   
```r
url <- "https://en.wikipedia.org/wiki/List_of_countries_by_COVID-19_cases"
page <- read_html(url)

```
  
3. Extract the data table

```r
table <- page %>% html_node("table") %>% html_table(fill = TRUE)
```

4. Clean and save the data

```r
covid_data <- table %>%
  select(Country = 1, Cases = 2) %>%
  filter(!is.na(Cases)) %>%
  mutate(Cases = as.numeric(gsub(",", "", Cases)))

write.csv(covid_data, "covid19.csv", row.names = FALSE)

```

5. Visualize the data
   
```r
ggplot(covid_data, aes(x = reorder(Country, -Cases), y = Cases)) +
  geom_bar(stat = "identity", fill = "#0073C2FF") +
  coord_flip() +
  labs(
    title = "COVID-19 Cases by Country",
    x = "Country",
    y = "Number of Cases"
  ) +
  theme_minimal()


```

## ğŸ“‚ Project Structure

The project directory contains the following files:



ğŸ“Œ **Details**:  
- **IBM Project.R** â€“ Core script where the web scraping process is implemented using `rvest`, data is processed with `dplyr`, and visualizations are generated using `ggplot2`.  
- **covid19.csv** â€“ CSV file generated by the script containing the cleaned dataset.  
- **README.md** â€“ This documentation file, explaining the project, installation, and usage.  


## ğŸ”® Future Improvements
- Scrape multiple data tables (e.g., deaths, vaccination rates)
- Automate daily updates and store data historically
- Deploy as an interactive dashboard with Shiny

ğŸ™ Credits
Data source: Wikipedia â€“ List of countries by COVID-19 cases

Author: Navid
